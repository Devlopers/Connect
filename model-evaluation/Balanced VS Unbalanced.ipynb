{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Metrics\n",
    "Choosing the correct metric to judge our model is often one of the most important steps in running any machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Classes\n",
    "Let's take a look at some evalution metrics when we have an even even split between ground truth labels. We call this balanced classes. For our predictions, let's assume that our classifer gives everything a positive label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "balanced = np.concatenate([np.zeros(50),np.ones(50)]).astype(int)\n",
    "pred = np.ones(100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:    0s:  0, 1s: 100\n",
      "Ground truth: 0s: 50, 1s: 50\n"
     ]
    }
   ],
   "source": [
    "c_pred = Counter(pred)\n",
    "print \"Predicted:    0s: {:2}, 1s: {:2}\".format(c_pred[0],c_pred[1])\n",
    "c_true = Counter(balanced)\n",
    "print \"Ground truth: 0s: {:2}, 1s: {:2}\".format(c_true[0],c_true[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1 score: 0.666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy:\",metrics.accuracy_score(pred,balanced)\n",
    "print \"F1 score:\",metrics.f1_score(pred,balanced)\n",
    "print \"Precision:\", metrics.precision_score(pred,balanced)\n",
    "print \"Recall:\",metrics.recall_score(pred,balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accuracy reflects our intuition (at least mine). We know we got half the predictions wrong, so our accuracy is 50%. Precision is 1 because for each ground truth 1, we correctly guessed it as 1. Recall is .5 because we were too zealous with our postive labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced Classes\n",
    "Unbalanced classes occur when there is more of one label than the other(s).\n",
    "\n",
    "Let's see if our evaluation metrics match our intuitions in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0s: 99, 1s: 1:\n",
      "Ground truth: 0s: 90, 1s: 10:\n"
     ]
    }
   ],
   "source": [
    "unbalanced = np.concatenate([np.zeros(90),np.ones(10)]).astype(int)\n",
    "pred = np.concatenate([np.zeros(99),np.ones(1)]).astype(int)\n",
    "c_pred = Counter(pred)\n",
    "print \"Predicted: 0s: {}, 1s: {}:\".format(c_pred[0],c_pred[1])\n",
    "c_true = Counter(unbalanced)\n",
    "print \"Ground truth: 0s: {}, 1s: {}:\".format(c_true[0],c_true[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here will make 90% of our data the negative label, with only 10% being positive. Perhaps the positive label represents cases of fraud at a bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "F1 score: 0.181818181818\n",
      "Precision: 0.1\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy:\",metrics.accuracy_score(pred,unbalanced)\n",
    "print \"F1 score:\",metrics.f1_score(pred,unbalanced)\n",
    "print \"Precision:\", metrics.precision_score(pred,unbalanced)\n",
    "print \"Recall:\",metrics.recall_score(pred,unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the outcome we wanted? We only correctly guessed 1/10 cases of fraud, but our accuracy is over 90%.\n",
    " **Accuracy is misleading in unbalanced classifcation problems.** In this case precision is a much better metric of how our classifier is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
